{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: Yu Ying Chen\n",
    "\n",
    "Student ID: 108060030\n",
    "\n",
    "GitHub ID: ARui-tw\n",
    "\n",
    "Kaggle name: imissina.com\n",
    "\n",
    "Kaggle private scoreboard snapshot: [Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2022-Lab2-master Repo](https://github.com/keziatamus/DM2022-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm2022-isa5810-lab2-homework) regarding Emotion Recognition on Twitter by this link https://www.kaggle.com/t/2b0d14a829f340bc88d2660dc602d4bd. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n",
    "    Submit your last submission __BEFORE the deadline (Nov. 22th 11:59 pm, Tuesday)_. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 25th 11:59 pm, Friday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
    "from sklearn.metrics import accuracy_score,matthews_corrcoef\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "SEED = 19\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == torch.device(\"cuda\"):\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data/data_pd.pkl')\n",
    "test = pd.read_pickle('data/test_pd.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anticipation', 'sadness', 'fear', 'joy', 'anger', 'trust',\n",
       "       'disgust', 'surprise'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "data['label_enc'] = labelencoder.fit_transform(data['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>label_enc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0x376b20</th>\n",
       "      <td>anticipation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2d5350</th>\n",
       "      <td>sadness</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1cd5b0</th>\n",
       "      <td>fear</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1d755c</th>\n",
       "      <td>joy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1fde89</th>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x33832f</th>\n",
       "      <td>trust</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2f4b5c</th>\n",
       "      <td>disgust</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1d5cff</th>\n",
       "      <td>surprise</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               emotion  label_enc\n",
       "tweet_id                         \n",
       "0x376b20  anticipation          1\n",
       "0x2d5350       sadness          5\n",
       "0x1cd5b0          fear          3\n",
       "0x1d755c           joy          4\n",
       "0x1fde89         anger          0\n",
       "0x33832f         trust          7\n",
       "0x2f4b5c       disgust          2\n",
       "0x1d5cff      surprise          6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['emotion','label_enc']].drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'emotion':'label_desc'},inplace=True)\n",
    "data.rename(columns={'label_enc':'label'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>crawldate</th>\n",
       "      <th>label_desc</th>\n",
       "      <th>identification</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0x376b20</th>\n",
       "      <td>Snapchat</td>\n",
       "      <td>Snapchat  People who post add me on Snapchat ...</td>\n",
       "      <td>391</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2d5350</th>\n",
       "      <td>freepress Trump Legacy CNN</td>\n",
       "      <td>freepress Trump Legacy CNN brianklaas As we se...</td>\n",
       "      <td>433</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1cd5b0</th>\n",
       "      <td></td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ LH</td>\n",
       "      <td>376</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1d755c</th>\n",
       "      <td>authentic Laugh Out Loud</td>\n",
       "      <td>authentic Laugh Out Loud  RISKshow The Kevin A...</td>\n",
       "      <td>120</td>\n",
       "      <td>2015-06-11 04:44:05</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2c91a8</th>\n",
       "      <td></td>\n",
       "      <td>Still waiting on those supplies Liscus LH</td>\n",
       "      <td>1021</td>\n",
       "      <td>2015-08-18 02:30:07</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            hashtags  \\\n",
       "tweet_id                               \n",
       "0x376b20                    Snapchat   \n",
       "0x2d5350  freepress Trump Legacy CNN   \n",
       "0x1cd5b0                               \n",
       "0x1d755c    authentic Laugh Out Loud   \n",
       "0x2c91a8                               \n",
       "\n",
       "                                                       text  score  \\\n",
       "tweet_id                                                             \n",
       "0x376b20   Snapchat  People who post add me on Snapchat ...    391   \n",
       "0x2d5350  freepress Trump Legacy CNN brianklaas As we se...    433   \n",
       "0x1cd5b0                  Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ LH    376   \n",
       "0x1d755c  authentic Laugh Out Loud  RISKshow The Kevin A...    120   \n",
       "0x2c91a8          Still waiting on those supplies Liscus LH   1021   \n",
       "\n",
       "                    crawldate    label_desc identification  label  \n",
       "tweet_id                                                           \n",
       "0x376b20  2015-05-23 11:42:47  anticipation          train      1  \n",
       "0x2d5350  2016-01-28 04:52:09       sadness          train      5  \n",
       "0x1cd5b0  2016-01-24 23:53:05          fear          train      3  \n",
       "0x1d755c  2015-06-11 04:44:05           joy          train      4  \n",
       "0x2c91a8  2015-08-18 02:30:07  anticipation          train      1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of data based on labels: \n",
      " 4    516017\n",
      "1    248935\n",
      "7    205478\n",
      "5    193437\n",
      "2    139101\n",
      "3     63999\n",
      "6     48729\n",
      "0     39867\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/arui/dm_venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m## Import BERT tokenizer, that is used to convert our text into tokens that corresponds to BERT library\u001b[39;00m\n\u001b[1;32m     11\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m,do_lower_case\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m input_ids \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39mencode(sent, add_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,max_length\u001b[39m=\u001b[39mMAX_LEN,pad_to_max_length\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences]\n\u001b[1;32m     13\u001b[0m labels \u001b[39m=\u001b[39m small_data\u001b[39m.\u001b[39mlabel\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mActual sentence before tokenization: \u001b[39m\u001b[39m\"\u001b[39m,sentences[\u001b[39m2\u001b[39m])\n",
      "Cell \u001b[0;32mIn [11], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m## Import BERT tokenizer, that is used to convert our text into tokens that corresponds to BERT library\u001b[39;00m\n\u001b[1;32m     11\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m,do_lower_case\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m input_ids \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39;49mencode(sent, add_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,max_length\u001b[39m=\u001b[39;49mMAX_LEN,pad_to_max_length\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences]\n\u001b[1;32m     13\u001b[0m labels \u001b[39m=\u001b[39m small_data\u001b[39m.\u001b[39mlabel\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mActual sentence before tokenization: \u001b[39m\u001b[39m\"\u001b[39m,sentences[\u001b[39m2\u001b[39m])\n",
      "File \u001b[0;32m~/dm_venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2259\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2222\u001b[0m \u001b[39m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2223\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2224\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m   2243\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:\n\u001b[1;32m   2244\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2245\u001b[0m \u001b[39m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2246\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2257\u001b[0m \u001b[39m            method).\u001b[39;00m\n\u001b[1;32m   2258\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2259\u001b[0m     encoded_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m   2260\u001b[0m         text,\n\u001b[1;32m   2261\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2262\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2263\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2264\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2265\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2266\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2267\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2268\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2269\u001b[0m     )\n\u001b[1;32m   2271\u001b[0m     \u001b[39mreturn\u001b[39;00m encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/dm_venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2667\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2657\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2658\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2659\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2660\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2664\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2665\u001b[0m )\n\u001b[0;32m-> 2667\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   2668\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2669\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2670\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2671\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2672\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2673\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2674\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2675\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2676\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2677\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2678\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2679\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2680\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2681\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2682\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2683\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2684\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2685\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2686\u001b[0m )\n",
      "File \u001b[0;32m~/dm_venv/lib/python3.8/site-packages/transformers/tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    641\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 649\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    650\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_for_model(\n\u001b[1;32m    653\u001b[0m     first_ids,\n\u001b[1;32m    654\u001b[0m     pair_ids\u001b[39m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    668\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    669\u001b[0m )\n",
      "File \u001b[0;32m~/dm_venv/lib/python3.8/site-packages/transformers/tokenization_utils.py:616\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_input_ids\u001b[39m(text):\n\u001b[1;32m    615\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 616\u001b[0m         tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenize(text, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    617\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    618\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(text[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m~/dm_venv/lib/python3.8/site-packages/transformers/tokenization_utils.py:517\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m     text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(pattern, \u001b[39mlambda\u001b[39;00m m: m\u001b[39m.\u001b[39mgroups()[\u001b[39m0\u001b[39m] \u001b[39mor\u001b[39;00m m\u001b[39m.\u001b[39mgroups()[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mlower(), text)\n\u001b[1;32m    516\u001b[0m no_split_token \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munique_no_split_tokens)\n\u001b[0;32m--> 517\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokens_trie\u001b[39m.\u001b[39;49msplit(text)\n\u001b[1;32m    518\u001b[0m \u001b[39m# [\"This is something\", \"<special_token_1>\", \"  else\"]\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[39mfor\u001b[39;00m i, token \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tokens):\n",
      "File \u001b[0;32m~/dm_venv/lib/python3.8/site-packages/transformers/tokenization_utils.py:221\u001b[0m, in \u001b[0;36mTrie.split\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    219\u001b[0m     states \u001b[39m=\u001b[39m {}\n\u001b[1;32m    220\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[39mfor\u001b[39;00m start \u001b[39min\u001b[39;00m to_remove:\n\u001b[1;32m    222\u001b[0m         \u001b[39mdel\u001b[39;00m states[start]\n\u001b[1;32m    224\u001b[0m \u001b[39m# If this character is a starting character within the trie\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39m# start keeping track of this partial match.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sentences = small_data.text.values\n",
    "\n",
    "#check distribution of data based on labels\n",
    "print(\"Distribution of data based on labels: \\n\", small_data.label.value_counts())\n",
    "\n",
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 256\n",
    "\n",
    "## Import BERT tokenizer, that is used to convert our text into tokens that corresponds to BERT library\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
    "input_ids = [tokenizer.encode(sent, add_special_tokens=True,max_length=MAX_LEN,pad_to_max_length=True) for sent in sentences]\n",
    "labels = small_data.label.values\n",
    "\n",
    "print(\"Actual sentence before tokenization: \",sentences[2])\n",
    "print(\"Encoded Input from dataset: \",input_ids[2])\n",
    "\n",
    "## Create attention mask\n",
    "attention_masks = []\n",
    "## Create a mask of 1 for all input tokens and 0 for all padding tokens\n",
    "attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n",
    "print(attention_masks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs,validation_inputs,train_labels,validation_labels = train_test_split(input_ids,labels,random_state=41,test_size=0.1)\n",
    "train_masks,validation_masks,_,_ = train_test_split(attention_masks,input_ids,random_state=41,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all our data into torch tensors, required data type for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "train_data = TensorDataset(train_inputs,train_masks,train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs,validation_masks,validation_labels)\n",
    "validation_sampler = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data,sampler=validation_sampler,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/arui/dm_venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=8).to(device)\n",
    "\n",
    "# Parameters:\n",
    "lr = 2e-5\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 3\n",
    "\n",
    "num_warmup_steps = 0\n",
    "num_training_steps = len(train_dataloader)*epochs\n",
    "\n",
    "### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
    "optimizer = AdamW(model.parameters(), lr=lr,eps=adam_epsilon,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8968a27b743e41d0b9598c859f727617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====================== Epoch 1 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  1.3333333333333333e-05\n",
      "\n",
      "\tAverage Training loss: 1.0666608575488914\n",
      "\n",
      "\tValidation Accuracy: 0.6496505642265699\n",
      "\n",
      "\tValidation MCC Accuracy: 0.5505549229091288\n",
      "<====================== Epoch 2 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  6.666666666666667e-06\n",
      "\n",
      "\tAverage Training loss: 0.8855636883391453\n",
      "\n",
      "\tValidation Accuracy: 0.6616001423651456\n",
      "\n",
      "\tValidation MCC Accuracy: 0.5661935263752111\n",
      "<====================== Epoch 3 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  0.0\n",
      "\n",
      "\tAverage Training loss: 0.7615322709681235\n",
      "\n",
      "\tValidation Accuracy: 0.6620499403322551\n",
      "\n",
      "\tValidation MCC Accuracy: 0.5691702111614892\n"
     ]
    }
   ],
   "source": [
    "## Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "learning_rate = []\n",
    "\n",
    "# Gradients gets accumulated by default\n",
    "model.zero_grad()\n",
    "\n",
    "i = 0\n",
    "\n",
    "# tnrange is a tqdm wrapper around the normal python range\n",
    "for _ in tqdm.notebook.trange(1,epochs+1,desc='Epoch'):\n",
    "  print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n",
    "  # Calculate total loss for this epoch\n",
    "  batch_loss = 0\n",
    "\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "    \n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    loss = outputs[0]\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip the norm of the gradients to 1.0\n",
    "    # Gradient clipping is not in AdamW anymore\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update learning rate schedule\n",
    "    scheduler.step()\n",
    "\n",
    "    # Clear the previous accumulated gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Update tracking variables\n",
    "    batch_loss += loss.item()\n",
    "\n",
    "  # Calculate the average loss over the training data.\n",
    "  avg_train_loss = batch_loss / len(train_dataloader)\n",
    "\n",
    "  #store the current learning rate\n",
    "  for param_group in optimizer.param_groups:\n",
    "    print(\"\\n\\tCurrent Learning rate: \",param_group['lr'])\n",
    "    learning_rate.append(param_group['lr'])\n",
    "    \n",
    "  train_loss_set.append(avg_train_loss)\n",
    "  print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n",
    "    \n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  eval_accuracy,eval_mcc_accuracy,nb_eval_steps = 0, 0, 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits[0].to('cpu').numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "    labels_flat = label_ids.flatten()\n",
    "    \n",
    "    df_metrics=pd.DataFrame({'Epoch':epochs,'Actual_class':labels_flat,'Predicted_class':pred_flat})\n",
    "    \n",
    "    tmp_eval_accuracy = accuracy_score(labels_flat,pred_flat)\n",
    "    tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n",
    "    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    eval_mcc_accuracy += tmp_eval_mcc_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print(F'\\n\\tValidation Accuracy: {eval_accuracy/nb_eval_steps}')\n",
    "  print(F'\\n\\tValidation MCC Accuracy: {eval_mcc_accuracy/nb_eval_steps}')\n",
    "\n",
    "  i += 1\n",
    "  model_save_folder = 'model/'\n",
    "  tokenizer_save_folder = 'tokenizer/'\n",
    "\n",
    "  path_model = f'./kaggle/working/{model_save_folder}_{i}'\n",
    "  path_tokenizer = f'./kaggle/working/{tokenizer_save_folder}_{i}'\n",
    "\n",
    "  #create the dir\n",
    "\n",
    "  !mkdir -p {path_model}\n",
    "  !mkdir -p {path_tokenizer}\n",
    "\n",
    "  ## Now let's save our model and tokenizer to a directory\n",
    "  model.save_pretrained(path_model)\n",
    "  tokenizer.save_pretrained(path_tokenizer)\n",
    "\n",
    "  model_save_name = 'fineTuneModel.pt'\n",
    "  path = path_model = f'./kaggle/working/{model_save_folder}_{i}/{model_save_name}'\n",
    "  torch.save(model.state_dict(),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>label_desc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0x376b20</th>\n",
       "      <td>1</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2d5350</th>\n",
       "      <td>5</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1cd5b0</th>\n",
       "      <td>3</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1d755c</th>\n",
       "      <td>4</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1fde89</th>\n",
       "      <td>0</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x33832f</th>\n",
       "      <td>7</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2f4b5c</th>\n",
       "      <td>2</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1d5cff</th>\n",
       "      <td>6</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label    label_desc\n",
       "tweet_id                     \n",
       "0x376b20      1  anticipation\n",
       "0x2d5350      5       sadness\n",
       "0x1cd5b0      3          fear\n",
       "0x1d755c      4           joy\n",
       "0x1fde89      0         anger\n",
       "0x33832f      7         trust\n",
       "0x2f4b5c      2       disgust\n",
       "0x1d5cff      6      surprise"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['label','label_desc']].drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## emotion labels\n",
    "label2int = {\n",
    "  \"anticipation\": 1,\n",
    "  \"sadness\": 5,\n",
    "  \"fear\": 3,\n",
    "  \"joy\": 4,\n",
    "  \"anger\": 0,\n",
    "  \"trust\": 7,\n",
    "  \"disgust\": 2,\n",
    "  \"surprise\": 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_folder = 'model/'\n",
    "tokenizer_save_folder = 'tokenizer/'\n",
    "\n",
    "i = 4\n",
    "\n",
    "path_model = F'./kaggle/working/{model_save_folder}_{i}'\n",
    "path_tokenizer = F'./kaggle/working/{tokenizer_save_folder}_{i}'\n",
    "\n",
    "#create the dir\n",
    "\n",
    "!mkdir -p {path_model}\n",
    "!mkdir -p {path_tokenizer}\n",
    "\n",
    "## Now let's save our model and tokenizer to a directory\n",
    "model.save_pretrained(path_model)\n",
    "tokenizer.save_pretrained(path_tokenizer)\n",
    "\n",
    "model_save_name = 'fineTuneModel.pt'\n",
    "path = path_model = F'./kaggle/working/{model_save_folder}/{model_save_name}'\n",
    "torch.save(model.state_dict(),path);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load modles and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now load the model and tokenizer from the directory\n",
    "model_save_folder = 'model2/'\n",
    "tokenizer_save_folder = 'tokenizer2/'\n",
    "\n",
    "i = 1\n",
    "\n",
    "path_model = F'./kaggle/working/{model_save_folder}_{i}'\n",
    "path_tokenizer = F'./kaggle/working/{tokenizer_save_folder}_{i}'\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(path_model, num_labels=8).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(path_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>crawldate</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0x28b412</th>\n",
       "      <td>bibleverse</td>\n",
       "      <td>bibleverse  Confident of your obedience I writ...</td>\n",
       "      <td>232</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2de201</th>\n",
       "      <td></td>\n",
       "      <td>Trust is not the same as faith A friend is so...</td>\n",
       "      <td>989</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x218443</th>\n",
       "      <td>materialism money possessions</td>\n",
       "      <td>materialism money possessions  When do you hav...</td>\n",
       "      <td>66</td>\n",
       "      <td>2015-09-09 09:22:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2939d5</th>\n",
       "      <td>Gods Plan Gods Work</td>\n",
       "      <td>Gods Plan Gods Work  God woke you up now chas...</td>\n",
       "      <td>104</td>\n",
       "      <td>2015-10-10 14:33:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x26289a</th>\n",
       "      <td></td>\n",
       "      <td>In these tough times who do YOU turn to as yo...</td>\n",
       "      <td>310</td>\n",
       "      <td>2016-10-23 08:49:50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               hashtags  \\\n",
       "tweet_id                                  \n",
       "0x28b412                     bibleverse   \n",
       "0x2de201                                  \n",
       "0x218443  materialism money possessions   \n",
       "0x2939d5            Gods Plan Gods Work   \n",
       "0x26289a                                  \n",
       "\n",
       "                                                       text  score  \\\n",
       "tweet_id                                                             \n",
       "0x28b412  bibleverse  Confident of your obedience I writ...    232   \n",
       "0x2de201   Trust is not the same as faith A friend is so...    989   \n",
       "0x218443  materialism money possessions  When do you hav...     66   \n",
       "0x2939d5   Gods Plan Gods Work  God woke you up now chas...    104   \n",
       "0x26289a   In these tough times who do YOU turn to as yo...    310   \n",
       "\n",
       "                    crawldate emotion identification  \n",
       "tweet_id                                              \n",
       "0x28b412  2017-12-25 04:39:20     NaN           test  \n",
       "0x2de201  2016-01-08 17:18:59     NaN           test  \n",
       "0x218443  2015-09-09 09:22:55     NaN           test  \n",
       "0x2939d5  2015-10-10 14:33:26     NaN           test  \n",
       "0x26289a  2016-10-23 08:49:50     NaN           test  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test the model\n",
    "test = pd.read_pickle('data/test_pd.pkl')\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## emotion labels\n",
    "label2int = {\n",
    "  \"anticipation\": 1,\n",
    "  \"sadness\": 5,\n",
    "  \"fear\": 3,\n",
    "  \"joy\": 4,\n",
    "  \"anger\": 0,\n",
    "  \"trust\": 7,\n",
    "  \"disgust\": 2,\n",
    "  \"surprise\": 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/arui/dm_venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "\n",
    "sentences = test.text.values\n",
    "\n",
    "input_ids = [tokenizer.encode(sent, add_special_tokens=True,max_length=MAX_LEN,pad_to_max_length=True) for sent in sentences]\n",
    "\n",
    "## Create attention mask\n",
    "attention_masks = []\n",
    "## Create a mask of 1 for all input tokens and 0 for all padding tokens\n",
    "attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "\n",
    "test_dataset = TensorDataset(input_ids, attention_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12875\n",
      "[array([1, 7, 4, 1, 7, 4, 3, 0, 4, 5, 4, 1, 5, 4, 4, 4, 4, 2, 4, 4, 5, 4,\n",
      "       4, 5, 4, 4, 2, 0, 4, 4, 5, 5]), array([5, 4, 5, 5, 5, 4, 4, 4, 1, 1, 4, 4, 1, 4, 5, 4, 2, 4, 4, 5, 2, 4,\n",
      "       5, 4, 5, 5, 2, 6, 5, 7, 7, 2]), array([4, 1, 4, 1, 4, 4, 4, 4, 5, 1, 1, 5, 4, 5, 4, 5, 4, 7, 2, 4, 4, 4,\n",
      "       4, 5, 5, 4, 1, 1, 4, 2, 7, 5]), array([5, 4, 1, 2, 4, 2, 7, 5, 2, 4, 5, 5, 1, 1, 7, 2, 1, 4, 4, 4, 5, 5,\n",
      "       2, 7, 5, 5, 5, 4, 2, 1, 5, 5]), array([5, 1, 5, 4, 5, 2, 4, 1, 4, 4, 1, 4, 7, 2, 1, 4, 4, 1, 5, 2, 4, 7,\n",
      "       5, 4, 5, 2, 5, 4, 4, 3, 4, 2]), array([4, 4, 7, 1, 5, 5, 5, 4, 7, 5, 5, 5, 5, 4, 4, 7, 2, 7, 2, 2, 4, 5,\n",
      "       5, 5, 4, 7, 5, 6, 5, 3, 4, 4]), array([1, 5, 5, 7, 2, 1, 1, 2, 4, 5, 5, 1, 1, 4, 4, 3, 5, 5, 4, 5, 2, 5,\n",
      "       4, 5, 5, 2, 4, 5, 4, 2, 1, 4]), array([3, 5, 1, 4, 2, 1, 2, 4, 1, 1, 0, 4, 5, 4, 5, 4, 7, 4, 5, 4, 4, 7,\n",
      "       4, 7, 5, 1, 5, 4, 2, 4, 4, 4]), array([1, 5, 3, 5, 5, 5, 4, 5, 1, 2, 1, 5, 7, 7, 5, 4, 5, 5, 4, 4, 4, 2,\n",
      "       7, 2, 5, 7, 1, 2, 5, 5, 4, 2]), array([4, 4, 4, 4, 5, 5, 4, 2, 4, 4, 5, 5, 4, 4, 5, 6, 2, 5, 4, 5, 5, 5,\n",
      "       4, 4, 5, 2, 5, 1, 4, 4, 7, 2])]\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "for batch in test_dataloader:\n",
    "\t# Add batch to GPU\n",
    "\tbatch = tuple(t.to(device) for t in batch)\n",
    "\t# Unpack the inputs from our dataloader\n",
    "\tb_input_ids, b_input_mask = batch\n",
    "\t# Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "\twith torch.no_grad():\n",
    "\t\t# Forward pass, calculate logit predictions\n",
    "\t\tlogits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "\t# Move logits and labels to CPU\n",
    "\tlogits = logits[0].to('cpu').numpy()\n",
    "\tpred_flat = np.argmax(logits, axis=1).flatten()\n",
    "\ta.append(pred_flat)\n",
    "\n",
    "print(len(a))\n",
    "print(a[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>crawldate</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0x28b412</th>\n",
       "      <td>bibleverse</td>\n",
       "      <td>bibleverse  Confident of your obedience I writ...</td>\n",
       "      <td>232</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2de201</th>\n",
       "      <td></td>\n",
       "      <td>Trust is not the same as faith A friend is so...</td>\n",
       "      <td>989</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x218443</th>\n",
       "      <td>materialism money possessions</td>\n",
       "      <td>materialism money possessions  When do you hav...</td>\n",
       "      <td>66</td>\n",
       "      <td>2015-09-09 09:22:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2939d5</th>\n",
       "      <td>Gods Plan Gods Work</td>\n",
       "      <td>Gods Plan Gods Work  God woke you up now chas...</td>\n",
       "      <td>104</td>\n",
       "      <td>2015-10-10 14:33:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x26289a</th>\n",
       "      <td></td>\n",
       "      <td>In these tough times who do YOU turn to as yo...</td>\n",
       "      <td>310</td>\n",
       "      <td>2016-10-23 08:49:50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               hashtags  \\\n",
       "tweet_id                                  \n",
       "0x28b412                     bibleverse   \n",
       "0x2de201                                  \n",
       "0x218443  materialism money possessions   \n",
       "0x2939d5            Gods Plan Gods Work   \n",
       "0x26289a                                  \n",
       "\n",
       "                                                       text  score  \\\n",
       "tweet_id                                                             \n",
       "0x28b412  bibleverse  Confident of your obedience I writ...    232   \n",
       "0x2de201   Trust is not the same as faith A friend is so...    989   \n",
       "0x218443  materialism money possessions  When do you hav...     66   \n",
       "0x2939d5   Gods Plan Gods Work  God woke you up now chas...    104   \n",
       "0x26289a   In these tough times who do YOU turn to as yo...    310   \n",
       "\n",
       "                    crawldate emotion identification  label  \n",
       "tweet_id                                                     \n",
       "0x28b412  2017-12-25 04:39:20     NaN           test      1  \n",
       "0x2de201  2016-01-08 17:18:59     NaN           test      7  \n",
       "0x218443  2015-09-09 09:22:55     NaN           test      4  \n",
       "0x2939d5  2015-10-10 14:33:26     NaN           test      1  \n",
       "0x26289a  2016-10-23 08:49:50     NaN           test      7  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output result\n",
    "test['label'] = np.concatenate(a)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>crawldate</th>\n",
       "      <th>identification</th>\n",
       "      <th>label</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0x28b412</th>\n",
       "      <td>bibleverse</td>\n",
       "      <td>bibleverse  Confident of your obedience I writ...</td>\n",
       "      <td>232</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2de201</th>\n",
       "      <td></td>\n",
       "      <td>Trust is not the same as faith A friend is so...</td>\n",
       "      <td>989</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>test</td>\n",
       "      <td>7</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x218443</th>\n",
       "      <td>materialism money possessions</td>\n",
       "      <td>materialism money possessions  When do you hav...</td>\n",
       "      <td>66</td>\n",
       "      <td>2015-09-09 09:22:55</td>\n",
       "      <td>test</td>\n",
       "      <td>4</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2939d5</th>\n",
       "      <td>Gods Plan Gods Work</td>\n",
       "      <td>Gods Plan Gods Work  God woke you up now chas...</td>\n",
       "      <td>104</td>\n",
       "      <td>2015-10-10 14:33:26</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x26289a</th>\n",
       "      <td></td>\n",
       "      <td>In these tough times who do YOU turn to as yo...</td>\n",
       "      <td>310</td>\n",
       "      <td>2016-10-23 08:49:50</td>\n",
       "      <td>test</td>\n",
       "      <td>7</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               hashtags  \\\n",
       "id                                        \n",
       "0x28b412                     bibleverse   \n",
       "0x2de201                                  \n",
       "0x218443  materialism money possessions   \n",
       "0x2939d5            Gods Plan Gods Work   \n",
       "0x26289a                                  \n",
       "\n",
       "                                                       text  score  \\\n",
       "id                                                                   \n",
       "0x28b412  bibleverse  Confident of your obedience I writ...    232   \n",
       "0x2de201   Trust is not the same as faith A friend is so...    989   \n",
       "0x218443  materialism money possessions  When do you hav...     66   \n",
       "0x2939d5   Gods Plan Gods Work  God woke you up now chas...    104   \n",
       "0x26289a   In these tough times who do YOU turn to as yo...    310   \n",
       "\n",
       "                    crawldate identification  label       emotion  \n",
       "id                                                                 \n",
       "0x28b412  2017-12-25 04:39:20           test      1  anticipation  \n",
       "0x2de201  2016-01-08 17:18:59           test      7         trust  \n",
       "0x218443  2015-09-09 09:22:55           test      4           joy  \n",
       "0x2939d5  2015-10-10 14:33:26           test      1  anticipation  \n",
       "0x26289a  2016-10-23 08:49:50           test      7         trust  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change label to label_desc\n",
    "inv_map = {v: k for k, v in label2int.items()}\n",
    "\n",
    "test['label_desc'] = test['label'].map(lambda x: inv_map[x])\n",
    "test.index.names = ['id']\n",
    "test.drop(columns=['emotion'],inplace=True)\n",
    "test.rename(columns={'label_desc':'emotion'},inplace=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(f'output_{i}.csv', columns = ['emotion'], index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('dm_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40e6b3af4460c5e981561514de903b2e99ebc4820fad5c77527bced5bbf015f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
